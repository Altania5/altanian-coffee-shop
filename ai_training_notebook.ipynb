{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Espresso AI Training Notebook\n",
        "\n",
        "This notebook provides advanced training capabilities for the Espresso AI system using TensorFlow, scikit-learn, and other ML libraries for sophisticated model development and analysis.\n",
        "\n",
        "## Features:\n",
        "- **Real Machine Learning Models**: Neural networks, regression, classification\n",
        "- **Advanced Feature Engineering**: Comprehensive feature extraction and selection\n",
        "- **Model Evaluation**: Cross-validation, performance metrics, visualization\n",
        "- **Data Analysis**: Statistical analysis, pattern recognition, correlation studies\n",
        "- **Export Models**: Save trained models for production use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"📚 Libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading and Preprocessing\n",
        "# This cell simulates loading coffee log data from your database\n",
        "# In practice, you would connect to MongoDB and fetch real data\n",
        "\n",
        "def create_sample_data(n_samples=1000):\n",
        "    \"\"\"Create realistic sample coffee log data for training\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    data = {\n",
        "        'grindSize': np.random.normal(15, 3, n_samples).clip(5, 25),\n",
        "        'extractionTime': np.random.normal(30, 5, n_samples).clip(15, 50),\n",
        "        'temperature': np.random.normal(93, 2, n_samples).clip(85, 96),\n",
        "        'inWeight': np.random.normal(18, 2, n_samples).clip(12, 25),\n",
        "        'outWeight': np.random.normal(36, 4, n_samples).clip(20, 50),\n",
        "        'usedPuckScreen': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),\n",
        "        'usedWDT': np.random.choice([0, 1], n_samples, p=[0.4, 0.6]),\n",
        "        'usedPreInfusion': np.random.choice([0, 1], n_samples, p=[0.5, 0.5]),\n",
        "        'preInfusionTime': np.random.normal(5, 2, n_samples).clip(0, 15),\n",
        "        'roastLevel': np.random.choice(['light', 'medium', 'dark'], n_samples),\n",
        "        'processMethod': np.random.choice(['washed', 'natural', 'honey'], n_samples)\n",
        "    }\n",
        "    \n",
        "    # Calculate derived features\n",
        "    data['ratio'] = data['outWeight'] / data['inWeight']\n",
        "    data['flowRate'] = data['outWeight'] / data['extractionTime']\n",
        "    \n",
        "    # Generate realistic shot quality based on parameters\n",
        "    quality = np.zeros(n_samples)\n",
        "    for i in range(n_samples):\n",
        "        base_quality = 5\n",
        "        \n",
        "        # Extraction time bonus\n",
        "        if 25 <= data['extractionTime'][i] <= 35:\n",
        "            base_quality += 1\n",
        "        elif data['extractionTime'][i] < 20 or data['extractionTime'][i] > 40:\n",
        "            base_quality -= 1\n",
        "            \n",
        "        # Ratio bonus\n",
        "        if 1.8 <= data['ratio'][i] <= 2.2:\n",
        "            base_quality += 1\n",
        "        elif data['ratio'][i] < 1.5 or data['ratio'][i] > 2.5:\n",
        "            base_quality -= 1\n",
        "            \n",
        "        # Temperature bonus\n",
        "        if 90 <= data['temperature'][i] <= 95:\n",
        "            base_quality += 0.5\n",
        "            \n",
        "        # Technique bonuses\n",
        "        if data['usedPuckScreen'][i]:\n",
        "            base_quality += 0.5\n",
        "        if data['usedWDT'][i]:\n",
        "            base_quality += 0.5\n",
        "        if data['usedPreInfusion'][i]:\n",
        "            base_quality += 0.5\n",
        "            \n",
        "        # Add some randomness\n",
        "        base_quality += np.random.normal(0, 0.5)\n",
        "        \n",
        "        quality[i] = np.clip(base_quality, 1, 10)\n",
        "    \n",
        "    data['shotQuality'] = quality\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Create sample dataset\n",
        "df = create_sample_data(1000)\n",
        "print(f\"📊 Created dataset with {len(df)} samples\")\n",
        "print(f\"📈 Quality distribution:\")\n",
        "print(df['shotQuality'].value_counts().sort_index())\n",
        "print(f\"\\n📊 Dataset shape: {df.shape}\")\n",
        "print(f\"📋 Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Visualization and Analysis\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Quality distribution\n",
        "plt.subplot(2, 3, 1)\n",
        "df['shotQuality'].hist(bins=10, alpha=0.7, color='brown')\n",
        "plt.title('Shot Quality Distribution')\n",
        "plt.xlabel('Quality Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Extraction time vs Quality\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.scatter(df['extractionTime'], df['shotQuality'], alpha=0.6, color='green')\n",
        "plt.title('Extraction Time vs Quality')\n",
        "plt.xlabel('Extraction Time (s)')\n",
        "plt.ylabel('Quality Score')\n",
        "\n",
        "# Ratio vs Quality\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.scatter(df['ratio'], df['shotQuality'], alpha=0.6, color='orange')\n",
        "plt.title('Ratio vs Quality')\n",
        "plt.xlabel('Ratio (out/in)')\n",
        "plt.ylabel('Quality Score')\n",
        "\n",
        "# Temperature vs Quality\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.scatter(df['temperature'], df['shotQuality'], alpha=0.6, color='red')\n",
        "plt.title('Temperature vs Quality')\n",
        "plt.xlabel('Temperature (°C)')\n",
        "plt.ylabel('Quality Score')\n",
        "\n",
        "# Grind size vs Quality\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.scatter(df['grindSize'], df['shotQuality'], alpha=0.6, color='purple')\n",
        "plt.title('Grind Size vs Quality')\n",
        "plt.xlabel('Grind Size')\n",
        "plt.ylabel('Quality Score')\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.subplot(2, 3, 6)\n",
        "correlation_matrix = df[['grindSize', 'extractionTime', 'temperature', 'inWeight', 'outWeight', 'ratio', 'shotQuality']].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Feature Correlation Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical summary\n",
        "print(\"📊 Statistical Summary:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering and Preprocessing\n",
        "def prepare_features(df):\n",
        "    \"\"\"Prepare features for machine learning\"\"\"\n",
        "    # Encode categorical variables\n",
        "    le_roast = LabelEncoder()\n",
        "    le_process = LabelEncoder()\n",
        "    \n",
        "    df_encoded = df.copy()\n",
        "    df_encoded['roastLevel_encoded'] = le_roast.fit_transform(df['roastLevel'])\n",
        "    df_encoded['processMethod_encoded'] = le_process.fit_transform(df['processMethod'])\n",
        "    \n",
        "    # Select features for training\n",
        "    feature_columns = [\n",
        "        'grindSize', 'extractionTime', 'temperature', 'inWeight', 'outWeight',\n",
        "        'usedPuckScreen', 'usedWDT', 'usedPreInfusion', 'preInfusionTime',\n",
        "        'roastLevel_encoded', 'processMethod_encoded', 'ratio', 'flowRate'\n",
        "    ]\n",
        "    \n",
        "    X = df_encoded[feature_columns]\n",
        "    y = df_encoded['shotQuality']\n",
        "    \n",
        "    return X, y, feature_columns\n",
        "\n",
        "# Prepare features\n",
        "X, y, feature_columns = prepare_features(df)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"📊 Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"📊 Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"📊 Features: {len(feature_columns)}\")\n",
        "print(f\"📋 Feature names: {feature_columns}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Traditional Machine Learning Models\n",
        "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate multiple ML models\"\"\"\n",
        "    \n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Ridge Regression': Ridge(alpha=1.0),\n",
        "        'Lasso Regression': Lasso(alpha=0.1),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "        'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"🔄 Training {name}...\")\n",
        "        \n",
        "        # Train model\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        # Cross-validation score\n",
        "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
        "        cv_mae = -cv_scores.mean()\n",
        "        \n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'r2': r2,\n",
        "            'cv_mae': cv_mae,\n",
        "            'predictions': y_pred\n",
        "        }\n",
        "        \n",
        "        print(f\"✅ {name} - MAE: {mae:.3f}, RMSE: {rmse:.3f}, R²: {r2:.3f}, CV MAE: {cv_mae:.3f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Train traditional ML models\n",
        "ml_results = train_and_evaluate_models(X_train, X_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deep Learning with TensorFlow/Keras\n",
        "def create_deep_learning_model(input_shape):\n",
        "    \"\"\"Create a deep neural network for coffee quality prediction\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(1, activation='linear')  # Regression output\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mse',\n",
        "        metrics=['mae', 'mse']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create and train deep learning model\n",
        "print(\"🧠 Creating Deep Learning Model...\")\n",
        "dl_model = create_deep_learning_model(X_train_scaled.shape[1])\n",
        "dl_model.summary()\n",
        "\n",
        "# Train the model\n",
        "print(\"🔄 Training Deep Learning Model...\")\n",
        "history = dl_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate deep learning model\n",
        "dl_predictions = dl_model.predict(X_test_scaled).flatten()\n",
        "dl_mae = mean_absolute_error(y_test, dl_predictions)\n",
        "dl_rmse = np.sqrt(mean_squared_error(y_test, dl_predictions))\n",
        "dl_r2 = r2_score(y_test, dl_predictions)\n",
        "\n",
        "print(f\"✅ Deep Learning Model - MAE: {dl_mae:.3f}, RMSE: {dl_rmse:.3f}, R²: {dl_r2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Comparison and Visualization\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training history for deep learning model\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    # Loss\n",
        "    ax1.plot(history.history['loss'], label='Training Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_title('Model Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # MAE\n",
        "    ax2.plot(history.history['mae'], label='Training MAE')\n",
        "    ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
        "    ax2.set_title('Model MAE')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('MAE')\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def compare_model_performance(ml_results, dl_mae, dl_rmse, dl_r2):\n",
        "    \"\"\"Compare performance of all models\"\"\"\n",
        "    # Add deep learning results\n",
        "    ml_results['Deep Learning'] = {\n",
        "        'mae': dl_mae,\n",
        "        'rmse': dl_rmse,\n",
        "        'r2': dl_r2\n",
        "    }\n",
        "    \n",
        "    # Create comparison DataFrame\n",
        "    comparison_data = []\n",
        "    for name, results in ml_results.items():\n",
        "        comparison_data.append({\n",
        "            'Model': name,\n",
        "            'MAE': results['mae'],\n",
        "            'RMSE': results['rmse'],\n",
        "            'R²': results['r2']\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    comparison_df = comparison_df.sort_values('MAE')\n",
        "    \n",
        "    print(\"📊 Model Performance Comparison:\")\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Plot comparison\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # MAE comparison\n",
        "    ax1.bar(comparison_df['Model'], comparison_df['MAE'], color='skyblue')\n",
        "    ax1.set_title('Mean Absolute Error Comparison')\n",
        "    ax1.set_ylabel('MAE')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # RMSE comparison\n",
        "    ax2.bar(comparison_df['Model'], comparison_df['RMSE'], color='lightcoral')\n",
        "    ax2.set_title('Root Mean Square Error Comparison')\n",
        "    ax2.set_ylabel('RMSE')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # R² comparison\n",
        "    ax3.bar(comparison_df['Model'], comparison_df['R²'], color='lightgreen')\n",
        "    ax3.set_title('R² Score Comparison')\n",
        "    ax3.set_ylabel('R²')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Compare all models\n",
        "comparison_df = compare_model_performance(ml_results, dl_mae, dl_rmse, dl_r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Export and Production Integration\n",
        "def export_best_model(comparison_df, ml_results, dl_model, scaler, feature_columns):\n",
        "    \"\"\"Export the best performing model for production use\"\"\"\n",
        "    \n",
        "    # Get best model based on MAE\n",
        "    best_model_name = comparison_df.iloc[0]['Model']\n",
        "    print(f\"🏆 Best Model: {best_model_name}\")\n",
        "    \n",
        "    if best_model_name == 'Deep Learning':\n",
        "        # Save TensorFlow model\n",
        "        dl_model.save('best_coffee_quality_model.h5')\n",
        "        print(\"✅ Deep Learning model saved as 'best_coffee_quality_model.h5'\")\n",
        "        \n",
        "        # Save scaler\n",
        "        import joblib\n",
        "        joblib.dump(scaler, 'coffee_scaler.pkl')\n",
        "        print(\"✅ Scaler saved as 'coffee_scaler.pkl'\")\n",
        "        \n",
        "        # Save feature columns\n",
        "        import json\n",
        "        with open('feature_columns.json', 'w') as f:\n",
        "            json.dump(feature_columns, f)\n",
        "        print(\"✅ Feature columns saved as 'feature_columns.json'\")\n",
        "        \n",
        "    else:\n",
        "        # Save sklearn model\n",
        "        best_model = ml_results[best_model_name]['model']\n",
        "        import joblib\n",
        "        joblib.dump(best_model, 'best_coffee_quality_model.pkl')\n",
        "        joblib.dump(scaler, 'coffee_scaler.pkl')\n",
        "        \n",
        "        with open('feature_columns.json', 'w') as f:\n",
        "            json.dump(feature_columns, f)\n",
        "        \n",
        "        print(f\"✅ {best_model_name} model saved as 'best_coffee_quality_model.pkl'\")\n",
        "        print(\"✅ Scaler saved as 'coffee_scaler.pkl'\")\n",
        "        print(\"✅ Feature columns saved as 'feature_columns.json'\")\n",
        "\n",
        "def create_production_predictor():\n",
        "    \"\"\"Create a production-ready predictor function\"\"\"\n",
        "    \n",
        "    predictor_code = '''\n",
        "# Production Coffee Quality Predictor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "class CoffeeQualityPredictor:\n",
        "    def __init__(self, model_path, scaler_path, features_path):\n",
        "        self.model = joblib.load(model_path)\n",
        "        self.scaler = joblib.load(scaler_path)\n",
        "        with open(features_path, 'r') as f:\n",
        "            self.feature_columns = json.load(f)\n",
        "    \n",
        "    def predict_quality(self, shot_data):\n",
        "        \"\"\"Predict coffee shot quality\"\"\"\n",
        "        # Prepare feature vector\n",
        "        features = np.array([[\n",
        "            shot_data.get('grindSize', 15),\n",
        "            shot_data.get('extractionTime', 30),\n",
        "            shot_data.get('temperature', 93),\n",
        "            shot_data.get('inWeight', 18),\n",
        "            shot_data.get('outWeight', 36),\n",
        "            shot_data.get('usedPuckScreen', 0),\n",
        "            shot_data.get('usedWDT', 0),\n",
        "            shot_data.get('usedPreInfusion', 0),\n",
        "            shot_data.get('preInfusionTime', 0),\n",
        "            shot_data.get('roastLevel_encoded', 1),\n",
        "            shot_data.get('processMethod_encoded', 0),\n",
        "            shot_data.get('ratio', 2.0),\n",
        "            shot_data.get('flowRate', 1.2)\n",
        "        ]])\n",
        "        \n",
        "        # Scale features\n",
        "        features_scaled = self.scaler.transform(features)\n",
        "        \n",
        "        # Make prediction\n",
        "        quality = self.model.predict(features_scaled)[0]\n",
        "        return max(1, min(10, round(quality)))\n",
        "\n",
        "# Usage example:\n",
        "# predictor = CoffeeQualityPredictor('best_coffee_quality_model.pkl', 'coffee_scaler.pkl', 'feature_columns.json')\n",
        "# quality = predictor.predict_quality({\n",
        "#     'grindSize': 15,\n",
        "#     'extractionTime': 30,\n",
        "#     'temperature': 93,\n",
        "#     'inWeight': 18,\n",
        "#     'outWeight': 36,\n",
        "#     'usedPuckScreen': 1,\n",
        "#     'usedWDT': 1,\n",
        "#     'usedPreInfusion': 0,\n",
        "#     'preInfusionTime': 0,\n",
        "#     'roastLevel_encoded': 1,\n",
        "#     'processMethod_encoded': 0,\n",
        "#     'ratio': 2.0,\n",
        "#     'flowRate': 1.2\n",
        "# })\n",
        "'''\n",
        "    \n",
        "    with open('coffee_quality_predictor.py', 'w') as f:\n",
        "        f.write(predictor_code)\n",
        "    \n",
        "    print(\"✅ Production predictor code saved as 'coffee_quality_predictor.py'\")\n",
        "\n",
        "# Export best model\n",
        "export_best_model(comparison_df, ml_results, dl_model, scaler, feature_columns)\n",
        "\n",
        "# Create production predictor\n",
        "create_production_predictor()\n",
        "\n",
        "print(\"\\n🎯 Training Complete! Your AI model is ready for production.\")\n",
        "print(\"📁 Files created:\")\n",
        "print(\"   - best_coffee_quality_model.h5/pkl (trained model)\")\n",
        "print(\"   - coffee_scaler.pkl (feature scaler)\")\n",
        "print(\"   - feature_columns.json (feature names)\")\n",
        "print(\"   - coffee_quality_predictor.py (production code)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
